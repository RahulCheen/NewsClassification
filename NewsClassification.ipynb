{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle BBC News Classification project\n",
    "Rahul Cheeniyil\n",
    "\n",
    "## Description\n",
    "This project aims to classify various news articles in to the categories business, entertainment, politics, sport or tech based on article content. I will be comparing a couple supervised learning models to an unsupervised model to see how each performs on this task. The supervised models I'll train are a Support Vector Machine and a Miltinomial Naive bayes model. The unsupervised model I'll use is Non-Negative Matrix Factorization. Below are all the imports we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.decomposition import NMF, TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "The first step is to check out our data and see what it looks like. The data is fairly simple. There are 1490 columns broken up into 3 columns. Column 1 is an integer article ID used to uniquely identify each article. Column 2 contains the text contents of the article. Column 3 contains the correct classification of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1490 entries, 0 to 1489\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   ArticleId  1490 non-null   int64 \n",
      " 1   Text       1490 non-null   object\n",
      " 2   Category   1490 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 35.1+ KB\n",
      "None\n",
      "   ArticleId                                               Text  Category\n",
      "0       1833  worldcom ex-boss launches defence lawyers defe...  business\n",
      "1        154  german business confidence slides german busin...  business\n",
      "2       1101  bbc poll indicates economic gloom citizens in ...  business\n",
      "3       1976  lifestyle  governs mobile choice  faster  bett...      tech\n",
      "4        917  enron bosses in $168m payout eighteen former e...  business\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('BBC News Train.csv')\n",
    "print(train_df.info())\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "There are a few things to do before I can apply my target models to the data. First up, we'll need to encode category labels into numeric values. We'll then split the training data into train and validate sets. Finally, we'll need to actually encode the text content to enable training. To do this, I use Term Frequency-Inverse Document Frequency (TF-IDF). This encoding basically measures how frequently a word appears in a document and reduces the importance of words that appear frequently (e.g. grammatical components as articles or prepositions). I use a maximum feature number of 5,000 to limit our encoding of each article to a 5,000-dimensional space. I could optimize this number of features through some testing, but I think 5,000 is a good starting point to balance memory usage and the fact that we're only working with ~1500 documents. If I had a few million documents, I would probably use 50,000 dimensions and might need to build a faster computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Labels are encoded as the following: {'business': 0, 'entertainment': 1, 'politics': 2, 'sport': 3, 'tech': 4}\n"
     ]
    }
   ],
   "source": [
    "# Encode our category labels into numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['Category_encoded'] = label_encoder.fit_transform(train_df['Category'])\n",
    "print(f' Labels are encoded as the following: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}')\n",
    "\n",
    "# Split data into train and validate sets using a standard 80/20 split\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df['Text'],\n",
    "                                                  train_df['Category_encoded'],\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=42)\n",
    "\n",
    "# Encode text into TF-IDF features\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = vectorizer.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "We'll start with the Naive Bayes model. This model assumes that words in a document appear independently. It assigns a probability of a document of belonging to each class and chooses the most likely one. I'm not doing any hyperparameter tuning here, just using the base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Naive Bayes model has an accuracy of 97.315%\n",
      "\n",
      " Performance report for the Naive Bayes model:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.97      0.97      0.97        75\n",
      "entertainment       1.00      1.00      1.00        46\n",
      "     politics       0.93      0.95      0.94        56\n",
      "        sport       0.98      1.00      0.99        63\n",
      "         tech       0.98      0.95      0.96        58\n",
      "\n",
      "     accuracy                           0.97       298\n",
      "    macro avg       0.97      0.97      0.97       298\n",
      " weighted avg       0.97      0.97      0.97       298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "nb_preds = nb_model.predict(X_val_tfidf)\n",
    "\n",
    "# Evaluate model\n",
    "nb_accuracy = accuracy_score(y_val, nb_preds)\n",
    "nb_report = classification_report(y_val, nb_preds, target_names=label_encoder.classes_)\n",
    "print(f'The Naive Bayes model has an accuracy of {round(nb_accuracy, 5)*100}%')\n",
    "print(\"\\n Performance report for the Naive Bayes model:\")\n",
    "print(nb_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine\n",
    "A SVM model finds the optimal hyperplane that separates the different classes. Since text data is high dimensional (5,000 dimensions as I chose earlier), I'll use a linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Support Vector Machine has an accuracy of 96.98%\n",
      "\n",
      " Performance report for the Support Vector Machine model:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.96      0.99      0.97        75\n",
      "entertainment       0.96      0.98      0.97        46\n",
      "     politics       0.93      0.95      0.94        56\n",
      "        sport       1.00      1.00      1.00        63\n",
      "         tech       1.00      0.93      0.96        58\n",
      "\n",
      "     accuracy                           0.97       298\n",
      "    macro avg       0.97      0.97      0.97       298\n",
      " weighted avg       0.97      0.97      0.97       298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "svm_model = SVC(kernel=\"linear\")\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "svm_preds = svm_model.predict(X_val_tfidf)\n",
    "\n",
    "# Evaluate model\n",
    "svm_accuracy = accuracy_score(y_val, svm_preds)\n",
    "svm_report = classification_report(y_val, svm_preds, target_names=label_encoder.classes_)\n",
    "print(f'The Support Vector Machine has an accuracy of {round(svm_accuracy, 5)*100}%')\n",
    "print(\"\\n Performance report for the Support Vector Machine model:\")\n",
    "print(svm_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Negative Matrix factorization\n",
    "I start with NMF here, but NMF might not be the best choice of model. Given our large, mostly sparse matrices that are typical of text data vectorized with TF-IDF, NMF is pretty computationally expensive. Selecting even 500 components took about 50x longer to run on my machine than the SVM and still only correctly classified articles about 2/3 of the time. Accuracy would likely improve with additional components or additional iterations for convergence, but we can probably do better with a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The NMF model has an accuracy of 67.785%\n",
      "\n",
      " Performance report for the NMF model:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.80      0.87      0.83        75\n",
      "entertainment       1.00      0.67      0.81        46\n",
      "     politics       0.96      0.48      0.64        56\n",
      "        sport       0.45      1.00      0.62        63\n",
      "         tech       0.89      0.28      0.42        58\n",
      "\n",
      "     accuracy                           0.68       298\n",
      "    macro avg       0.82      0.66      0.66       298\n",
      " weighted avg       0.81      0.68      0.67       298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build model using 500 components\n",
    "nmf = NMF(n_components=500, random_state=42)\n",
    "X_train_nmf = nmf.fit_transform(X_train_tfidf)\n",
    "X_val_nmf = nmf.transform(X_val_tfidf)\n",
    "\n",
    "# Train model\n",
    "logreg_nmf = LogisticRegression(max_iter=1000)\n",
    "logreg_nmf.fit(X_train_nmf, y_train)\n",
    "nmf_preds = logreg_nmf.predict(X_val_nmf)\n",
    "\n",
    "# Evaluate the model\n",
    "nmf_accuracy = accuracy_score(y_val, nmf_preds)\n",
    "nmf_report = classification_report(y_val, nmf_preds, target_names=label_encoder.classes_)\n",
    "\n",
    "print(f'The NMF model has an accuracy of {round(nmf_accuracy, 5)*100}%')\n",
    "print(\"\\n Performance report for the NMF model:\")\n",
    "print(nmf_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncated SVD (Latent Semantic Analysis)\n",
    "Instead of NMF, Truncated SVD might be more appropriate here. Truncated SVD should be more performant on our larger matrices here. Compared to NMF which should have a time complexity of O(NDkI), SVD should have a complexity of O(NDk) where N is the number of documents, D is the number of features, k is the number of latent dimensions, and I is the number of iterations for convergence. Additionally, because textual data is largely sparse, SVD will be more memory efficient because it doesn't need to convert to dense matrices like NMF does. From the results below, the SVD model performs both much faster than the NMF model and with much better accuracy around 96% which is more in line with the earlier supervised models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The SVD model has an accuracy of 96.309%\n",
      "\n",
      " Performance report for the SVD model:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.96      0.97      0.97        75\n",
      "entertainment       0.96      0.98      0.97        46\n",
      "     politics       0.93      0.93      0.93        56\n",
      "        sport       0.98      1.00      0.99        63\n",
      "         tech       0.98      0.93      0.96        58\n",
      "\n",
      "     accuracy                           0.96       298\n",
      "    macro avg       0.96      0.96      0.96       298\n",
      " weighted avg       0.96      0.96      0.96       298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build model also using 500 components\n",
    "svd = TruncatedSVD(n_components=500, random_state=42)\n",
    "X_train_svd = svd.fit_transform(X_train_tfidf)\n",
    "X_val_svd = svd.transform(X_val_tfidf)\n",
    "\n",
    "# Train model\n",
    "logreg_svd = LogisticRegression(max_iter=1000)\n",
    "logreg_svd.fit(X_train_svd, y_train)\n",
    "svd_preds = logreg_svd.predict(X_val_svd)\n",
    "\n",
    "# Evaluate the model\n",
    "svd_accuracy = accuracy_score(y_val, svd_preds)\n",
    "svd_report = classification_report(y_val, svd_preds, target_names=label_encoder.classes_)\n",
    "\n",
    "print(f'The SVD model has an accuracy of {round(svd_accuracy, 5)*100}%')\n",
    "print(\"\\n Performance report for the SVD model:\")\n",
    "print(svd_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Below are a few visualizations comparing and contrasting the results produced by the models I trained in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'               precision    recall  f1-score   support\\n\\n     business       0.96      0.97      0.97        75\\nentertainment       0.96      0.98      0.97        46\\n     politics       0.93      0.93      0.93        56\\n        sport       0.98      1.00      0.99        63\\n         tech       0.98      0.93      0.96        58\\n\\n     accuracy                           0.96       298\\n    macro avg       0.96      0.96      0.96       298\\n weighted avg       0.96      0.96      0.96       298\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to parse the report strings into dictionaries because I'm not manually typing out all of those results.\n",
    "import re\n",
    "\n",
    "def parse_classification_report(report_str: str) -> dict:\n",
    "    report_dict = {}\n",
    "    lines = report_str.strip().split('\\n')\n",
    "\n",
    "    # Regex pattern to match report item values\n",
    "    pattern = re.compile(r'^\\s*(\\S+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+(\\d+)')\n",
    "\n",
    "    for line in lines:\n",
    "        if match := pattern.match(line):\n",
    "            category, precision, recall, f1_score, support = match.groups()\n",
    "            report_dict[category] = {\n",
    "                \"precision\": float(precision),\n",
    "                \"recall\": float(recall),\n",
    "                \"f1-score\": float(f1_score),\n",
    "                \"support\": int(support)\n",
    "            }\n",
    "\n",
    "    return report_dict\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
